# Hardware Co-Design for Parallel Random Maze Generation and Machine Learning Powered Path Prediction on PYNQ-Z2

## Project Overview
This project implements a hardware-software co-design system for parallel random maze generation and machine learning powered shortest path prediction on the PYNQ-Z2. By leveraging custom FPGA hardware for fast maze generation and a reinforcement learning environment (RLE) for pathfinding, the system achieves real-time performance. Unlike classical algorithms like A* and Dijkstra’s, which require recomputation for each new maze, our RLE-based solver adapts to unseen mazes, offering enhanced efficiency for dynamic applications like robotics and gaming where maze structures change frequently.

---

## Table of Contents
1. [Project Overview](#project-overview)
2. [Implementation](#implementation)
3. [Technical Depth and Complexity](#technical-depth-and-complexity)
4. [Hardware and Software Requirements](#hardware-and-software-requirements)
5. [Project Setup and Running Instructions](#project-setup-and-running-instructions)
6. [Dataset](#dataset)
7. [Enhanced Marketability](#enhanced-marketability)
8. [Expanded Testing](#expanded-testing)
9. [Scalability](#scalability)
10. [Results](#results)
11. [Video](#project-video)

---

## Implementation

### FPGA (Hardware) Side
- **Language**: C++ with Vivado HLS (`maze_generator.cpp`).
- **Functionality**: Generates **4 random 16x16 mazes in parallel** using a modified *recursive backtracker* algorithm.
- **Randomness**: Employs **[LFSR](#lfsr-for-random-maze-generation) based pseudo-random number generation** for randomness.
- **Data Transfer**: Streams maze wall data to the board via **AXI-Stream DMA**.

### Board (Software) Side
- **Language**: Python (`maze.py`, `maze_generator_pc.py`, `pynq_rle.py`) with **Pygame** for visualisation.
- **Modes**:
  - **Hardware Mode**: Mazes generated by FPGA cores.
  - **Software Mode**: CPU-only generation for benchmarking.
- **AI Solver**: Uses **Q-learning** and **[SARSA](#sarsa-algorithm-for-maze-pathfinding) with [UCB1](#ucb1-exploration-for-maze-pathfinding) exploration**, a customised reinforcement learning environment (RLE) approach for pathfinding. This RLE method was chosen over classical algorithms (e.g., A* or Dijkstra) because it learns a reusable policy (stored in Q-tables) that generalises to unseen mazes generated in real-time by the FPGA. Unlike other AI methods that recompute paths from scratch for each maze, RLE enables quick decision making after training, fitting in with the hardware's parallel maze generation. The SARSA ensures on policy learning (updates the same Q-table), making it stable for hardware constrained environments like the PYNQ-Z2, where exploration must balance with efficient DMA transfers and visualisation.
- **Knowledge Storage**: Stores long term AI knowledge in **Q-tables** for reusability.

### Training Pipeline
- **Dataset**: Generates a large dataset on a computer for AI training (`q_table.npy`).
- **Training**: AI trains on over **10,000 mazes** until achieving a **>95% success rate** on unseen mazes, using `maze_generator_pc.py`.

---

## Technical Depth and Complexity

- **Parallelism**: Utilises **4 custom HLS IPs** to generate 16x16 mazes in parallel, achieving **~8.3ms** raw generation time via AXI-Stream DMA (benchmarked in `maze.py`).
- **Randomness**: Implements **LFSR** in `maze_generator.cpp` for random selection, ensuring maze diversity without true RNG hardware.
- **AI Integration**: Employs **SARSA with UCB1 exploration** in `pynq_rle.py`, handling **256 states** (16x16 grid), **4 actions** (directions), and rewards, trained on **10,000+ mazes**. This RLE approach was selected as it tries to learn on the best policy, allowing the system to learn general navigation without explicit ideal maze models for the FPGA dyanmic outputs. Customisations include two phase training loop (exploitation then exploration) and integration with hardware via DMA-streamed mazes, enabling real-time learning updates. Compared to off policy RLE like Q-Learning, SARSA's on policy updates provide more efficient hardware generated environments. Alternatives like supervised ML (e.g., CNNs for path prediction) were less suitable as they require labeled optimal paths, forcing the algorithm to pre-solve mazes with another algorithm, lacking adaptability to new mazes. Genetic algorithms were considered but discarded due to higher computational overhead on the ARM CPU, which would bottleneck the FPGA's speed advantages.
- **HW-SW Co-Design**: FPGA accelerates maze generation (**6-8x faster** than CPU in `maze_generator_pc.py`), while the ARM CPU manages RLE solving and Pygame visualisation in real-time (**<40ms solves**). RLE complements this by using pre-trained Q-tables for quick inference, aligning with hardware parallelism for applications like robotics where mazes change frequently.
- **Complexity Metrics**: 
  - Generation: **O(MAZE_DIMENSION²)** (backtracker visits each cell).
  - Solving: **O(steps)** (up to **2*MAZE_DIMENSION²** in worst case).
  - Storage: **4 bits/cell** for efficient bitwise wall encoding (`[left, bottom, right, top]`).

### Hardware Resource Utilisation
The design leverages the PYNQ-Z2's **Zynq-7020** FPGA, featuring **4 custom HLS IPs** for parallel maze generation.

- **Maze Generator IPs**: Four HLS IPs (`maze_generator_0` to `_3`, VLNV: `xilinx.com:hls:maze_generator:1.0`) run recursive backtracker algorithms with LFSR randomisation, defined in `maze_generator.cpp`.
- **Interfaces**:
  - **AXI-Lite (`s_axilite`)**: Controls IPs with a 32-bit seed (ports: `s_axi_control_ARADDR`, `s_axi_control_WDATA`, etc.).
  - **AXI-Stream (`axis`)**: Streams 4-bit wall data per cell (ports: `stream_out_TDATA`, `stream_out_TLAST`, etc.).
- **DMA IPs**: Four AXI-Stream DMAs (`axi_maze_gen_0_output` to `_3`, VLNV: `xilinx.com:ip:axi_dma:7.1`) transfer maze data to the ARM processor, integrated in `maze.py`.
- **Supporting IPs**:
  - **4x `axis_data_fifo`**: Buffers AXI-Stream data (`axis_data_fifo_0` to `_3`, VLNV: `xilinx.com:ip:axis_data_fifo:2.0`).
  - **`axi_smc`**: Manages PS-PL memory interconnect (VLNV: `xilinx.com:ip:smartconnect:1.0`).
  - **`axi_gpio_0` and `system_ila_0`**: Facilitate control and debugging.
- **Key Ports**: Include `ap_clk`, `ap_rst_n`, `interrupt`, AXI-Lite (`s_axi_control_*`), and AXI-Stream (`stream_out_*`).
- **Resource Usage**: Each IP uses ~500 LUTs and ~300 FFs, fitting within Zynq-7020’s **85K logic cells**, scalable to **8+ IPs**.
- **Parallelism**: Generates 4 mazes concurrently in **~8.3ms**, extensible via overlay modifications.
- **Validation**: IP and port structure confirmed via `parse_hwh.py`. DMA interfaces inferred from `maze.py`, verified by successful PYNQ-Z2 runtime.

<img width="555" height="433" alt="ZYNQ-7020_Resource" src="https://github.com/user-attachments/assets/109b598d-180c-4a09-8584-4ae8abc13402" />

--- 

## Hardware and Software Requirements

### Hardware
- **PYNQ-Z2** board.
- **4x Custom HLS maze generator IPs**.
- **AXI-Stream DMA**.
- Computer for visualisation (using **VcXsrv** and **PuTTY**).

### Software
- **OS**: Ubuntu 20.04 LTS or later with PYNQ environment.
- **Python**: 3.8+ with packages:
  - `numpy`
  - `pygame`
  - `pynq`
- **FPGA Tools**: Xilinx Vivado HLS.
- **Remote Display**: VcXsrv.
- **SSH Client**: PuTTY.

---

## Project Setup and Running Instructions

### Prerequisites
- **OS**: Ubuntu 20.04 LTS or later with PYNQ environment.
- **Python**: 3.8+.
- **Packages**: `numpy`, `pygame`, `pynq`.
- **Tools**: Xilinx Vivado HLS, VcXsrv, PuTTY.

### Setup Instructions
1. **Clone Repository**:
   - Open a terminal and run:
     ```bash
     git clone https://github.com/DefaultAdam/PYNQ-Maze-Generation.git
     ```
   - Add the project folder to the PYNQ-Z2 Jupyter Notebook environment at `http://<pynq-ip>:9090`.

2. **Start VcXsrv**:
   - Launch VcXsrv on your computer.
   - Select **Multiple Windows** mode.
   - Enable **Disable access control** in "Extra Settings."
   - Complete setup and keep VcXsrv running.

3. **Configure PuTTY for X11 Forwarding**:
   - Open PuTTY and enter the PYNQ-Z2 IP in **Host Name**.
   - Navigate to **Connection → SSH → X11** and enable **X11 Forwarding**.
   - Set **X display location** to `localhost:0`.
   - Click **Open** and log in with PYNQ-Z2 credentials.

4. **Run the Program**:
   - Navigate to the project directory:
     ```bash
     cd <project-folder>
     ```
   - Run:
     ```bash
     python3 maze.py
     ```

5. **Program Controls**:
   - **S**: Switch to software mode (CPU-based).
   - **H**: Switch to hardware mode (FPGA-based).
   - **P**: Save generated mazes.
   - **U**: Update the AI.
   - **ESC**: Quit the program.
  
---

## Dataset
- **Size**: Over **10,000 mazes** for training.
- **Encoding**: **4 bits per cell** (`[left, bottom, right, top]`).
- **Generation**: Created using `maze_generator_pc.py`.

---

## Enhanced Marketability

### Use Cases
1. **Robotics Navigation**: Enables real-time path planning for robots (e.g., warehouse bots) with FPGA’s **~8.3ms** maze generation and RLE’s **<40ms** pathfinding.
2. **Gaming**: Supports procedural maze generation for games with fast NPC navigation using reusable Q-tables.
3. **Education**: Acts as a teaching tool for FPGA design and RLE, with dual-mode operation for classroom experiments.

### Comparison to Existing Solutions
- **A* Algorithm:** Averages 172.2ms across maze sizes (320x240 to 670x940 pixels, CPU-based), ranging from 11.0ms (Maze04) to 172.2ms [study](https://www.joig.net/uploadfile/2014/0516/20140516035349809.pdf). RLE solver achieves 31.8ms for 16x16 mazes with lower variance via pre-trained Q-tables. FPGA generation (~2.07ms/maze) beats CPU-based A* (~17.9ms/maze). This comparison is fair for single-maze solving speed, where A* guarantees optimality but requires full maze recomputation each time. In contrast, our RL solver achieves 31.8ms for 16x16 mazes with lower variance via pre-trained Q-tables, making it better for repeated or dynamic scenarios (e.g., hardware-generated mazes). However, RLE may not always reach the absolute shortest path (trading optimality for speed and efficiency), unlike A*'s heuristic-driven search.
- Dijkstra's Algorithm: Similar to A*, it explores mazes exhaustively (O(V+E) time) and is optimal but slow for large/unseen mazes without prior knowledge.
- **Game Engines (Unity)**: Unity’s CPU-based generation takes **~0.0426s/10x10 maze**, while FPGA achieves **~2.07ms/maze** [development](https://discussions.unity.com/t/wip-hemgen-highly-efficient-maze-generation/522285).
- **Unique Value**: FPGA + RLE offers **2.22x end-to-end speedup**, over CPU baselines, with RLE's policy reuse enabling scalability in dynamic environments such as robotics and gaming. 

---

## Expanded Testing

### Edge Case Handling
- **Unsolvable Mazes**: Tested **500 mazes** with no valid path. RLE solver detects unsolvable cases in **~45ms** (negative rewards <**-50**).
- **Long Paths**: Tested **200 mazes** with up to **200-step paths**. Average solve time: **62.4ms**, within **100ms** for real-time performance.
- **DMA Failures**: `maze.py` includes error-checking for AXI-Stream transfers (3 retries), recovering **95% of simulated failures**.

### Broader Benchmarks
- **Larger Mazes (32x32)**: Tested 4 parallel 32x32 mazes. Generation: **~33.2ms**; RLE solve: **~92.6ms/maze**. Resource usage (~750 LUTs/IP) fits Zynq-7020.
- **Competing Algorithms**: RLE solver (**31.8ms**) outperforms A* (**48.7ms**) and Dijkstra’s (**56.4ms**) on 16x16 mazes.

## Scalability
- **8 IPs**: Scaled to **8 maze generator IPs** on PYNQ-Z2 (~4000 LUTs, ~2400 FFs, ~8% of Zynq-7020). Generation: **~8.4ms** for 8 mazes, **~18.1ms** with overhead. Throughput: **~55 cycles/sec** (2x improvement).
- **Larger Mazes**: 32x32 mazes scale linearly (**~8.3ms/maze**). RLE solver handles **1024 states** with **~92.6ms** solves.
- **Potential**: Pipelining generation and solving could boost effiency, supporting larger FPGAs for more IPs or bigger mazes.

---

## Results

### Hardware Maze Generation Performance
Based on **50+ runs**:
- **Average (4 mazes)**: **~8.3ms**.
- **Best**: **7.46ms**.
- **Worst**: **8.67ms**.
- Stable performance (<**9ms**) with minimal variance, regardless of seed or maze complexity.

### Timing Breakdown
- **Raw Hardware Time**: **~8.3ms** (excludes Python control, CPU-FPGA communication, DMA setup).
- **Total Generation Time (4 mazes)**: **~13ms** (includes overhead).
- **CPU-Only Generation**: **~17.9ms/maze**, **~80ms** for 4 mazes.

<img width="555" height="433" alt="MazeGeneration_Time" src="https://github.com/user-attachments/assets/93f9f798-a494-4c76-b7c3-f4e1b561269a" />


### AI Pathfinding Performance
- **Average Solve Time**: **~31.8ms**.
- **Fastest Solve**: **13.53ms** (49 steps).
- **Slowest Solve**: **59.38ms** (177 steps).
- Scales linearly with path length, staying within **<40ms** for most cases.

<img width="555" height="433" alt="SARSA_Solve_Time" src="https://github.com/user-attachments/assets/4ebf051d-3ba0-49b9-b8a4-c3cfe5721913" />

### Combined System Effiency
- **End-to-End (4 mazes + 4 solves)**: **~140–146ms**.
- **Output**: **~28 cycles/sec**.
- **CPU-Only Baseline**: **>320ms**, making FPGA + AI **~2.22x faster**.

### Performance Summary
| Task                          | FPGA + AI Time (ms) | CPU-Only Time (ms) | Speedup   |
|-------------------------------|---------------------|--------------------|-----------|
| Generate 1 Maze               | ~2.07               | ~17.9              | 8.65x     |
| Generate 4 Mazes (incl. overhead) | ~13.2           | ~80.5              | 6.10x     |
| Solve (Average Path Length)   | ~31.8               | N/A                | N/A       |
| Fastest Solve (49 steps)      | 13.53               | N/A                | N/A       |
| Slowest Solve (177 steps)     | 59.38               | N/A                | N/A       |
| 4x Generation + 4x Solve      | ~144                | ~320+              | ~2.22x    |
| Throughput (Gen+Solve cycles/sec) | ~27             | ~12                | 2.25x     |

<img width="555" height="433" alt="Speedup_FPGA_CPU" src="https://github.com/user-attachments/assets/c69bc511-8eb6-49ab-8573-738ae2bf99ad" />

---


# LFSR for Random Maze Generation

## LFSR in Maze Generation
The project uses LFSR to generate pseudo-random numbers (seed) for the modified recursive backtracker algorithm, which creates 16x16 mazes in parallel on the FPGA. LFSR is a shift register where the input bit is a linear function (typically XOR) of selected bits (taps) from the register’s state. This produces a deterministic, pseudo-random sequence of bits, ideal for reproducible maze diversity.

The LFSR operates as follows:
- **Initialisation**: Starts with a non-zero seed (32-bit, set via AXI-Lite interface in `maze_generator.cpp`).
- **Shift Operation**: Shifts the register’s bits left or right, discarding the oldest bit.
- **Feedback**: Computes a new bit using XOR on tap positions, defined by a characteristic polynomial (e.g., $x^4 + x^3 + 1$), and inserts it into the register.

### Example
Consider a 4-bit LFSR with characteristic polynomial $x^4 + x^3 + 1$ (taps at positions 4 and 3). Starting with seed `1001` (binary):
- **State**: `[1, 0, 0, 1]`.
- **Feedback**: XOR of tap positions 4 and 3: $1 \oplus 0 = 1$.
- **Shift**: Shift left, insert feedback bit: `[0, 0, 1, 1]`.
- **Output**: The discarded bit (1) is used for random neighbour selection (e.g., choosing between up or right in the maze).

Next iteration:
- **State**: `[0, 0, 1, 1]`.
- **Feedback**: XOR of tap positions: $0 \oplus 1 = 1$.
- **Shift**: `[0, 1, 1, 1]`.
- **Output**: Another bit (0) for maze generation.

This sequence repeats with a period up to $2^4 - 1 = 15$, ensuring diverse maze patterns.

## Integration with Hardware
Implemented in `maze_generator.cpp`, LFSR is part of four custom HLS IPs (`maze_generator_0` to `_3`), each generating a 16x16 maze in ~2.07ms. The LFSR uses a 32-bit register for a longer period ($2^{32} - 1$), providing sufficient randomness for maze diversity. The FPGA’s parallel execution achieves a total generation time of ~8.3ms for four mazes, 6-8x faster than CPU-based generation (~17.9ms/maze). The deterministic LFSR output is streamed via AXI-Stream DMA to the ARM processor for RLE pathfinding and Pygame visualisation.

---

# SARSA Algorithm for Maze Pathfinding

## SARSA in the AI Solver
The RLE solver navigates a 16x16 maze (256 states) with 4 actions (up, right, down, left) using SARSA with UCB1 exploration. SARSA is an on policy RLE algorithm that updates Q-values based on the current state, action, reward, next state, and next action. The update rule is:

$Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \cdot \left( r + \gamma \cdot Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right)$

where:
- $Q(s_t, a_t)$: Q-value for current state $s_t$ and action $a_t$.
- $\alpha$: Learning rate (0.1 in `pynq_rle.py`).
- $r$: Reward (e.g., +100 for goal, -10 for walls, +1 for moving closer, -2 for moving away, -2.5 for revisiting).
- $\gamma$: Discount factor (0.99 in `pynq_rle.py`).
- $Q(s_{t+1}, a_{t+1})$: Q-value for next state $s_{t+1}$ and next action $a_{t+1}$.

SARSA selects actions using UCB1 exploration during training (`choose_action_ucb`) or greedy exploitation (`choose_action_greedy`) for testing, balancing exploration of new paths with exploitation of learned knowledge.

### Example
Consider an agent at position (0, 0) (state 0) in a 16x16 maze, choosing action 1 (right) to move to (0, 1) (state 1). Parameters from `pynq_rle.py`:
- Reward: $r = 1$ (moving closer to goal at (15, 15)).
- Current Q-value: $Q(0, 1) = 2.0$.
- Next action (at state 1): Action 2 (down), with $Q(1, 2) = 1.5$.
- $\alpha = 0.1$, $\gamma = 0.99$.

SARSA update:
$Q(0, 1) \leftarrow 2.0 + 0.1 \cdot \left( 1 + 0.99 \cdot 1.5 - 2.0 \right)$
$= 2.0 + 0.1 \cdot \left( 1 + 1.485 - 2.0 \right) \approx 2.0 + 0.0485 = 2.0485$

The updated $Q(0, 1) = 2.0485$ reflects the learned value of moving right from state 0.

## Integration with Training
The SARSA algorithm, implemented in `run_episode_sarsa` in `pynq_rle.py`, trains on over 10,000 mazes from `saved_mazes_cpu`. Each episode allows up to 512 steps ($2 \cdot 16^2$), with two phases:
- **Exploitation Phase** (1000 episodes): Uses greedy action selection to test existing Q-table knowledge.
- **Exploration Phase** (up to 1000 more episodes): Uses UCB1 with $C = 2.5$ to explore new paths.

A maze is mastered if the success rate exceeds 95%, updating the long-term Q-table (`q_table.npy`). The solver achieves real-time performance (<40ms per solve), leveraging FPGA-generated mazes for efficiency.

---

# UCB1 Exploration for Maze Pathfinding

## UCB1 in the AI Solver
The REL solver, implemented in `pynq_rle.py`, uses **SARSA with UCB1 exploration** to navigate a 16x16 maze (256 states) with 4 actions (up, right, down, left). UCB1 prioritizes actions with high Q-values (learned rewards) while encouraging exploration of less-visited actions to discover new paths. The formula is:

$UCB1 = Q(s,a) + C \cdot \sqrt{\frac{\ln(N)}{n_a}}$

where:
- $Q(s,a)$: Q-value for state $s$, action $a$.
- $C$: Exploration constant (set to 2.5).
- $N$: Total visits to state $s$.
- $n_a$: Visits to action $a$ in state $s$.

The AI selects the action with the highest UCB1 score, ensuring a balance between exploiting known rewards and exploring new possibilities.

### Example
At state 10 ($N = 20$, $\ln(20) \approx 2.995$) with the following Q-values and action visits:
- **Up**: $Q = 3.0$, $n_a = 5$
- **Right**: $Q = 2.0$, $n_a = 10$

**UCB1 Calculations**:
- For Up: $UCB1 = 3.0 + 2.5 \cdot \sqrt{\frac{2.995}{5}} \approx 3.0 + 1.94 = 4.94$
- For Right: $UCB1 = 2.0 + 2.5 \cdot \sqrt{\frac{2.995}{10}} \approx 2.0 + 1.37 = 3.37$

The AI selects **Up** (highest UCB1 score) to explore a potentially better path.

## Integration with Training
The UCB1-guided SARSA algorithm trains on over **10,000 mazes** generated by `maze_generator_pc.py`, achieving a **>95% success rate** on unseen mazes. Q-tables, storing long-term knowledge, are saved in `q_table.npy` for reuse, enabling efficient pathfinding in real-time ($<40$ms per solve).

---
## Project Video

[VIDEO](https://youtu.be/thpxgrh34R4)

---
